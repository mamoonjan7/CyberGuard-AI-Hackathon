{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f429fbe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 14:19:21,483 - INFO - Using device: cuda\n",
      "2024-11-07 14:19:21,780 - INFO - Use pytorch device_name: cuda\n",
      "2024-11-07 14:19:21,780 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2024-11-07 14:19:24,289 - INFO - Preparing data...\n",
      "2024-11-07 14:19:24,289 - INFO - Loading datasets...\n",
      "2024-11-07 14:19:24,983 - INFO - Training set size: 93686\n",
      "2024-11-07 14:19:24,983 - INFO - Test set size: 31229\n",
      "2024-11-07 14:19:24,983 - INFO - \n",
      "Training set label distribution:\n",
      "2024-11-07 14:19:24,999 - INFO - Online Financial Fraud: 65459\n",
      "2024-11-07 14:19:25,000 - INFO - Online and Social Media Related Crime: 12733\n",
      "2024-11-07 14:19:25,000 - INFO - Cyber Attack/ Dependent Crimes: 3704\n",
      "2024-11-07 14:19:25,000 - INFO - RapeGang Rape RGRSexually Abusive Content: 2822\n",
      "2024-11-07 14:19:25,000 - INFO - Any Other Cyber Crime: 2154\n",
      "2024-11-07 14:19:25,000 - INFO - Sexually Obscene material: 1838\n",
      "2024-11-07 14:19:25,002 - INFO - Hacking  Damage to computercomputer system etc: 1710\n",
      "2024-11-07 14:19:25,002 - INFO - Sexually Explicit Act: 1552\n",
      "2024-11-07 14:19:25,003 - INFO - Cryptocurrency Crime: 490\n",
      "2024-11-07 14:19:25,004 - INFO - Online Gambling  Betting: 444\n",
      "2024-11-07 14:19:25,004 - INFO - Child Pornography CPChild Sexual Abuse Material CSAM: 379\n",
      "2024-11-07 14:19:25,005 - INFO - Online Cyber Trafficking: 183\n",
      "2024-11-07 14:19:25,005 - INFO - Cyber Terrorism: 161\n",
      "2024-11-07 14:19:25,006 - INFO - Ransomware: 56\n",
      "2024-11-07 14:19:25,006 - INFO - Report Unlawful Content: 1\n",
      "2024-11-07 14:19:25,017 - WARNING - Found 1 unseen labels in test set: {'Crime Against Women & Children'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb519b9e874c400eb3f5112ef4657c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429b1b903dd54da3929154df106a7195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae1ad5babe044cea2c3125fd6713a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b0e0ee747a430d840ea0040cafe394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620df4ed6c8c4fa3a14d838403f5a8d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c9e1223fd8498ababbae32c9be88c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac4b56c9fa74e6788d19e13b4e1c57c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc96a816dd34dbca7faa522542ad337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7191e49a0e8c413aacbc555e3694bbc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06f30d07de7466ab54ee982ebf6c0bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9be7d3fe1d43b498e8d394ea8ec1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749e53f1b3b64d52bad6592797b55b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5550216ad4f8454396092dd9ed2eb9b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d525a968dc4f01b7eb19692a84549a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976efadf0d4f46c393bf76fb4900fa0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c73fdd89adc44b188882b060dbf485e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 14:19:25,745 - INFO - Similarity between 'Crime Against Women & Children' and 'Online and Social Media Related Crime': 0.4543\n",
      "2024-11-07 14:19:25,747 - INFO - Similarity between 'Crime Against Women & Children' and 'Online Financial Fraud': 0.3502\n",
      "2024-11-07 14:19:25,748 - INFO - Similarity between 'Crime Against Women & Children' and 'Online Gambling  Betting': 0.3267\n",
      "2024-11-07 14:19:25,750 - INFO - Similarity between 'Crime Against Women & Children' and 'RapeGang Rape RGRSexually Abusive Content': 0.2141\n",
      "2024-11-07 14:19:25,751 - INFO - Similarity between 'Crime Against Women & Children' and 'Cyber Attack/ Dependent Crimes': 0.9750\n",
      "2024-11-07 14:19:25,752 - INFO - Similarity between 'Crime Against Women & Children' and 'Cryptocurrency Crime': 0.2977\n",
      "2024-11-07 14:19:25,753 - INFO - Similarity between 'Crime Against Women & Children' and 'Sexually Explicit Act': 0.3848\n",
      "2024-11-07 14:19:25,755 - INFO - Similarity between 'Crime Against Women & Children' and 'Sexually Obscene material': 0.3445\n",
      "2024-11-07 14:19:25,756 - INFO - Similarity between 'Crime Against Women & Children' and 'Hacking  Damage to computercomputer system etc': 0.4549\n",
      "2024-11-07 14:19:25,757 - INFO - Similarity between 'Crime Against Women & Children' and 'Any Other Cyber Crime': 0.2763\n",
      "2024-11-07 14:19:25,758 - INFO - Similarity between 'Crime Against Women & Children' and 'Cyber Terrorism': 0.3570\n",
      "2024-11-07 14:19:25,759 - INFO - Similarity between 'Crime Against Women & Children' and 'Child Pornography CPChild Sexual Abuse Material CSAM': 0.2589\n",
      "2024-11-07 14:19:25,760 - INFO - Similarity between 'Crime Against Women & Children' and 'Online Cyber Trafficking': 0.3132\n",
      "2024-11-07 14:19:25,762 - INFO - Similarity between 'Crime Against Women & Children' and 'Ransomware': 0.3917\n",
      "2024-11-07 14:19:25,763 - INFO - Similarity between 'Crime Against Women & Children' and 'Report Unlawful Content': 0.1180\n",
      "2024-11-07 14:19:25,764 - INFO - Mapping unknown label 'Crime Against Women & Children' to 'Cyber Attack/ Dependent Crimes' with similarity 0.9750\n",
      "2024-11-07 14:19:25,793 - INFO - Number of unique labels: 15\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-11-07 14:19:44,201 - INFO - Starting training process...\n",
      "2024-11-07 14:19:44,201 - INFO - Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca56e5fcfd542ba91115198e213bfad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Overall Progress:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 14:19:44,213 - INFO - \n",
      "Starting Epoch 1/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/3:   0%|                                                          | 0/11711 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 14:21:48,880 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 14:21:48,880 - INFO - New best model saved with accuracy: 23.68%\n",
      "2024-11-07 14:23:54,781 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 14:23:54,792 - INFO - New best model saved with accuracy: 43.53%\n",
      "2024-11-07 14:26:00,895 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 14:26:00,895 - INFO - New best model saved with accuracy: 52.64%\n",
      "2024-11-07 14:28:07,480 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 14:28:07,480 - INFO - New best model saved with accuracy: 56.79%\n",
      "2024-11-07 14:30:14,326 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 14:30:14,326 - INFO - New best model saved with accuracy: 59.25%\n",
      "2024-11-07 14:31:49,489 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 14:31:49,491 - INFO - New best model saved with accuracy: 60.93%\n",
      "2024-11-07 14:33:12,795 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 14:33:12,796 - INFO - New best model saved with accuracy: 62.31%\n",
      "2024-11-07 14:34:38,214 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 14:34:38,215 - INFO - New best model saved with accuracy: 63.71%\n",
      "2024-11-07 14:36:04,744 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 14:36:04,745 - INFO - New best model saved with accuracy: 65.39%\n",
      "2024-11-07 14:37:31,517 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 14:37:31,518 - INFO - New best model saved with accuracy: 67.00%\n",
      "2024-11-07 14:38:58,672 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 14:38:58,674 - INFO - New best model saved with accuracy: 68.30%\n",
      "2024-11-07 14:40:25,904 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 14:40:25,906 - INFO - New best model saved with accuracy: 69.42%\n",
      "2024-11-07 14:41:53,511 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 14:41:53,513 - INFO - New best model saved with accuracy: 70.33%\n",
      "2024-11-07 14:43:20,926 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 14:43:20,927 - INFO - New best model saved with accuracy: 71.13%\n",
      "2024-11-07 14:44:48,054 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 14:44:48,056 - INFO - New best model saved with accuracy: 71.90%\n",
      "2024-11-07 14:46:15,113 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 14:46:15,115 - INFO - New best model saved with accuracy: 72.52%\n",
      "2024-11-07 14:47:42,265 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 14:47:42,266 - INFO - New best model saved with accuracy: 73.18%\n",
      "2024-11-07 14:49:09,313 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 14:49:09,315 - INFO - New best model saved with accuracy: 73.66%\n",
      "2024-11-07 14:50:36,491 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 14:50:36,493 - INFO - New best model saved with accuracy: 74.15%\n",
      "2024-11-07 14:52:03,457 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 14:52:03,461 - INFO - New best model saved with accuracy: 74.56%\n",
      "2024-11-07 14:53:30,459 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 14:53:30,460 - INFO - New best model saved with accuracy: 74.94%\n",
      "2024-11-07 14:54:57,603 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 14:54:57,605 - INFO - New best model saved with accuracy: 75.30%\n",
      "2024-11-07 14:56:24,825 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 14:56:24,827 - INFO - New best model saved with accuracy: 75.66%\n",
      "2024-11-07 15:07:19,253 - INFO - Epoch 1 completed. Validation accuracy: 81.55%\n",
      "2024-11-07 15:07:19,775 - INFO - Model saved to checkpoint_epoch_1.pt\n",
      "2024-11-07 15:07:19,776 - INFO - Checkpoint saved: checkpoint_epoch_1.pt\n",
      "2024-11-07 15:07:19,779 - INFO - \n",
      "Starting Epoch 2/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/3:   0%|                                                          | 0/11711 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 15:08:46,753 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 15:08:46,753 - INFO - New best model saved with accuracy: 82.93%\n",
      "2024-11-07 15:10:13,681 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 15:10:13,683 - INFO - New best model saved with accuracy: 83.18%\n",
      "2024-11-07 15:14:32,205 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 15:14:32,208 - INFO - New best model saved with accuracy: 83.21%\n",
      "2024-11-07 15:28:51,429 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 15:28:51,432 - INFO - New best model saved with accuracy: 83.25%\n",
      "2024-11-07 15:30:17,617 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 15:30:17,619 - INFO - New best model saved with accuracy: 83.26%\n",
      "2024-11-07 15:36:01,149 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 15:36:01,150 - INFO - New best model saved with accuracy: 83.26%\n",
      "2024-11-07 15:40:19,731 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 15:40:19,733 - INFO - New best model saved with accuracy: 83.26%\n",
      "2024-11-07 15:50:55,913 - INFO - Epoch 2 completed. Validation accuracy: 84.16%\n",
      "2024-11-07 15:50:56,276 - INFO - Model saved to checkpoint_epoch_2.pt\n",
      "2024-11-07 15:50:56,278 - INFO - Checkpoint saved: checkpoint_epoch_2.pt\n",
      "2024-11-07 15:50:56,280 - INFO - \n",
      "Starting Epoch 3/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/3:   0%|                                                          | 0/11711 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 15:52:21,850 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 15:52:21,850 - INFO - New best model saved with accuracy: 83.83%\n",
      "2024-11-07 15:53:47,042 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 15:53:47,044 - INFO - New best model saved with accuracy: 84.13%\n",
      "2024-11-07 15:55:12,171 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 15:55:12,172 - INFO - New best model saved with accuracy: 84.28%\n",
      "2024-11-07 15:56:37,107 - INFO - Model saved to best_model.pt\n",
      "2024-11-07 15:56:37,109 - INFO - New best model saved with accuracy: 84.39%\n",
      "2024-11-07 16:35:13,387 - INFO - Epoch 3 completed. Validation accuracy: 84.65%\n",
      "2024-11-07 16:35:13,786 - INFO - Model saved to checkpoint_epoch_3.pt\n",
      "2024-11-07 16:35:13,788 - INFO - Checkpoint saved: checkpoint_epoch_3.pt\n",
      "2024-11-07 16:35:14,182 - INFO - Model saved to final_model.pt\n",
      "2024-11-07 16:35:14,184 - INFO - Final model saved\n",
      "2024-11-07 16:35:14,569 - INFO - Model saved to models\\cybercrime_classifier\\final_model.pt\n",
      "2024-11-07 16:35:14,571 - INFO - Evaluating model...\n",
      "2024-11-07 16:35:14,572 - INFO - Evaluating model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e65598c45214bed986b638ba560ae65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/3904 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 16:38:53,973 - INFO - \n",
      "Testing model with sample prediction...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diagnostics:\n",
      "Number of unique classes in predictions: 9\n",
      "Number of unique classes in true labels: 14\n",
      "Number of classes in label encoder: 15\n",
      "\n",
      "Class distribution in predictions:\n",
      "Class 0 (Any Other Cyber Crime): 556 samples\n",
      "Class 2 (Cryptocurrency Crime): 67 samples\n",
      "Class 3 (Cyber Attack/ Dependent Crimes): 1265 samples\n",
      "Class 5 (Hacking  Damage to computercomputer system etc): 556 samples\n",
      "Class 7 (Online Financial Fraud): 23252 samples\n",
      "Class 9 (Online and Social Media Related Crime): 4555 samples\n",
      "Class 11 (RapeGang Rape RGRSexually Abusive Content): 825 samples\n",
      "Class 13 (Sexually Explicit Act): 4 samples\n",
      "Class 14 (Sexually Obscene material): 149 samples\n",
      "\n",
      "Class distribution in true labels:\n",
      "Class 0 (Any Other Cyber Crime): 710 samples\n",
      "Class 1 (Child Pornography CPChild Sexual Abuse Material CSAM): 123 samples\n",
      "Class 2 (Cryptocurrency Crime): 167 samples\n",
      "Class 3 (Cyber Attack/ Dependent Crimes): 1303 samples\n",
      "Class 4 (Cyber Terrorism): 52 samples\n",
      "Class 5 (Hacking  Damage to computercomputer system etc): 592 samples\n",
      "Class 6 (Online Cyber Trafficking): 61 samples\n",
      "Class 7 (Online Financial Fraud): 21620 samples\n",
      "Class 8 (Online Gambling  Betting): 134 samples\n",
      "Class 9 (Online and Social Media Related Crime): 4336 samples\n",
      "Class 10 (Ransomware): 18 samples\n",
      "Class 11 (RapeGang Rape RGRSexually Abusive Content): 912 samples\n",
      "Class 13 (Sexually Explicit Act): 535 samples\n",
      "Class 14 (Sexually Obscene material): 666 samples\n",
      "\n",
      "Classification Report:\n",
      "                                                precision    recall  f1-score   support\n",
      "\n",
      "                         Any Other Cyber Crime     0.4964    0.3887    0.4360       710\n",
      "                          Cryptocurrency Crime     0.6269    0.2515    0.3590       167\n",
      "                Cyber Attack/ Dependent Crimes     1.0000    0.9708    0.9852      1303\n",
      "Hacking  Damage to computercomputer system etc     0.3327    0.3125    0.3223       592\n",
      "                        Online Financial Fraud     0.8917    0.9590    0.9241     21620\n",
      "         Online and Social Media Related Crime     0.5868    0.6165    0.6013      4336\n",
      "     RapeGang Rape RGRSexually Abusive Content     1.0000    0.9046    0.9499       912\n",
      "                         Sexually Explicit Act     0.0000    0.0000    0.0000       535\n",
      "                     Sexually Obscene material     0.3423    0.0766    0.1252       666\n",
      "\n",
      "                                     micro avg     0.8342    0.8447    0.8394     30841\n",
      "                                     macro avg     0.5863    0.4978    0.5225     30841\n",
      "                                  weighted avg     0.8080    0.8447    0.8229     30841\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "Labels: ['Any Other Cyber Crime', 'Cryptocurrency Crime', 'Cyber Attack/ Dependent Crimes', 'Hacking  Damage to computercomputer system etc', 'Online Financial Fraud', 'Online and Social Media Related Crime', 'RapeGang Rape RGRSexually Abusive Content', 'Sexually Explicit Act', 'Sexually Obscene material']\n",
      "[[  276     0     0    11   279   141     0     0     3]\n",
      " [    0    42     0     2   122     1     0     0     0]\n",
      " [    2     0  1265    22     3    11     0     0     0]\n",
      " [   16     1     0   185   277   113     0     0     0]\n",
      " [  146    23     0   129 20733   589     0     0     0]\n",
      " [   80     1     0   151  1410  2673     0     0    21]\n",
      " [    3     0     0     3    14    61   825     0     6]\n",
      " [    9     0     0    15   117   365     0     0    29]\n",
      " [    9     0     0    17    89   496     0     4    51]]\n",
      "\n",
      "Overall Accuracy: 83.42%\n",
      "\n",
      "Sample Prediction:\n",
      "Text: I received a call from someone claiming to be from SBI bank asking for my card details. \n",
      "        The...\n",
      "Predicted Category: Online Financial Fraud\n",
      "Confidence: 98.91%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Union, Optional\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import traceback\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Data paths\n",
    "        self.train_path = 'cleaned_cybercrime_data.csv'\n",
    "        self.test_path = 'cleaned_cybercrime_test_data.csv'\n",
    "        \n",
    "        # High-accuracy model parameters\n",
    "        self.model_name = 'distilbert-base-uncased'\n",
    "        self.max_length = 256     # Keep original length for accuracy\n",
    "        self.batch_size = 8      # Keep smaller batch size for stability\n",
    "        self.num_epochs = 3\n",
    "        self.learning_rate = 1e-5\n",
    "        self.warmup_ratio = 0.2\n",
    "        self.weight_decay = 0.01\n",
    "        \n",
    "        # Speed optimizations that won't hurt accuracy\n",
    "        self.gradient_accumulation_steps = 4\n",
    "        self.eval_steps = 500    # Evaluate less frequently\n",
    "        self.save_steps = 500\n",
    "        \n",
    "        # Device settings\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Add this line\n",
    "        \n",
    "        # Parallel processing settings\n",
    "        self.num_workers = 4 if self.device == 'cuda' else 2  # Adjust based on device\n",
    "        self.pin_memory = True if self.device == 'cuda' else False\n",
    "        \n",
    "        # Text preprocessing\n",
    "        self.max_vocab_size = 50000\n",
    "        self.text_column = 'crimeaditionalinfo'\n",
    "        self.label_column = 'category'\n",
    "        \n",
    "        # Output directory\n",
    "        self.output_dir = Path('models/cybercrime_classifier')\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "class TextPreprocessor:\n",
    "    \"\"\"Efficient text preprocessing for CPU\"\"\"\n",
    "    def __init__(self):\n",
    "        self.patterns = {\n",
    "            'url': re.compile(r'https?://\\S+|www\\.\\S+'),\n",
    "            'email': re.compile(r'\\S+@\\S+'),\n",
    "            'phone': re.compile(r'(\\+\\d{1,3}[-.]?)?\\d{3}[-.]?\\d{3}[-.]?\\d{4}'),\n",
    "            'amount': re.compile(r'(?:rs\\.?|inr|₹|\\$)\\s*\\d+(?:[,\\.]\\d+)*'),\n",
    "            'special_chars': re.compile(r'[^\\w\\s]'),\n",
    "            'extra_spaces': re.compile(r'\\s+')\n",
    "        }\n",
    "        \n",
    "        self.cache = {}\n",
    "        \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean text with caching for efficiency\"\"\"\n",
    "        if text in self.cache:\n",
    "            return self.cache[text]\n",
    "            \n",
    "        text = text.lower().strip()\n",
    "        \n",
    "        # Replace patterns with tokens\n",
    "        text = self.patterns['url'].sub('[URL]', text)\n",
    "        text = self.patterns['email'].sub('[EMAIL]', text)\n",
    "        text = self.patterns['phone'].sub('[PHONE]', text)\n",
    "        text = self.patterns['amount'].sub('[AMOUNT]', text)\n",
    "        \n",
    "        # Remove special characters and extra spaces\n",
    "        text = self.patterns['special_chars'].sub(' ', text)\n",
    "        text = self.patterns['extra_spaces'].sub(' ', text).strip()\n",
    "        \n",
    "        # Cache result\n",
    "        if len(self.cache) > 10000:  # Limit cache size\n",
    "            self.cache.clear()\n",
    "        self.cache[text] = text\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def batch_process(self, texts: List[str], batch_size: int = 1000) -> List[str]:\n",
    "        \"\"\"Process texts in batches\"\"\"\n",
    "        processed_texts = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            processed_batch = [self.clean_text(text) for text in batch]\n",
    "            processed_texts.extend(processed_batch)\n",
    "            \n",
    "        return processed_texts\n",
    "        \n",
    "        \n",
    "class SmartLabelEncoder:\n",
    "    \"\"\"Enhanced label encoder that handles unseen labels using semantic similarity\"\"\"\n",
    "    def __init__(self):\n",
    "        self.encoder = LabelEncoder()\n",
    "        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.label_embeddings = None\n",
    "        self.original_labels = None\n",
    "        self.mapping = {}\n",
    "        self.label_vectors = None\n",
    "\n",
    "    def fit(self, labels: List[str], texts: List[str]) -> 'SmartLabelEncoder':\n",
    "        \"\"\"Fit encoder and compute label embeddings\"\"\"\n",
    "        self.original_labels = list(set(labels))\n",
    "        self.encoder.fit(self.original_labels)\n",
    "        \n",
    "        # Create mapping of labels to their texts\n",
    "        label_texts = {}\n",
    "        for label, text in zip(labels, texts):\n",
    "            if label not in label_texts:\n",
    "                label_texts[label] = []\n",
    "            label_texts[label].append(text)\n",
    "        \n",
    "        # Create embeddings for each label's texts\n",
    "        self.label_vectors = {}\n",
    "        for label, texts in label_texts.items():\n",
    "            # Get text embeddings\n",
    "            text_embeddings = self.sentence_model.encode(texts[:5])  # Use up to 5 examples\n",
    "            # Calculate centroid\n",
    "            self.label_vectors[label] = np.mean(text_embeddings, axis=0)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _create_label_descriptions(self, labels: List[str], texts: List[str]) -> Dict[str, str]:\n",
    "        \"\"\"Create rich descriptions for each label based on its examples\"\"\"\n",
    "        descriptions = {}\n",
    "        for label, text in zip(labels, texts):\n",
    "            if label not in descriptions:\n",
    "                descriptions[label] = []\n",
    "            descriptions[label].append(text)\n",
    "        \n",
    "        # Combine examples for each label\n",
    "        for label in descriptions:\n",
    "            examples = descriptions[label][:5]  # Take up to 5 examples\n",
    "            descriptions[label] = f\"{label}: \" + \" \".join(examples)\n",
    "            \n",
    "        return descriptions\n",
    "    \n",
    "    def _create_label_vectors(self, descriptions: Dict[str, str]) -> Dict[str, Dict[str, np.ndarray]]:\n",
    "        \"\"\"Create enhanced vector representations of labels\"\"\"\n",
    "        vectors = {}\n",
    "        for label, texts in descriptions.items():\n",
    "            # Create embeddings for all texts\n",
    "            text_embeddings = self.sentence_model.encode(texts)\n",
    "            text_centroid = np.mean(text_embeddings, axis=0)\n",
    "            \n",
    "            vectors[label] = {\n",
    "                'text_embeddings': text_embeddings,\n",
    "                'text_centroid': text_centroid,\n",
    "            }\n",
    "        return vectors\n",
    "\n",
    "    \n",
    "    def _find_most_similar_label(self, unknown_label: str, texts: List[str]) -> str:\n",
    "        \"\"\"Find most similar known label using semantic similarity\"\"\"\n",
    "        # Create embedding for unknown label's texts\n",
    "        unknown_embeddings = self.sentence_model.encode(texts[:5])  # Use up to 5 examples\n",
    "        unknown_centroid = np.mean(unknown_embeddings, axis=0)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = {}\n",
    "        for known_label, known_embedding in self.label_vectors.items():\n",
    "            # Calculate cosine similarity\n",
    "            similarity = cosine_similarity(\n",
    "                unknown_centroid.reshape(1, -1),\n",
    "                known_embedding.reshape(1, -1)\n",
    "            )[0][0]\n",
    "            \n",
    "            similarities[known_label] = similarity\n",
    "            logger.info(f\"Similarity between '{unknown_label}' and '{known_label}': {similarity:.4f}\")\n",
    "        \n",
    "        # Get most similar label\n",
    "        most_similar = max(similarities.items(), key=lambda x: x[1])\n",
    "        logger.info(\n",
    "            f\"Mapping unknown label '{unknown_label}' to '{most_similar[0]}' \"\n",
    "            f\"with similarity {most_similar[1]:.4f}\"\n",
    "        )\n",
    "        return most_similar[0]\n",
    "    \n",
    "    def transform(self, labels: List[str], texts: List[str] = None) -> np.ndarray:\n",
    "        \"\"\"Transform labels, mapping unseen ones to most similar known labels\"\"\"\n",
    "        processed_labels = []\n",
    "        for i, label in enumerate(labels):\n",
    "            if label not in self.original_labels:\n",
    "                if label not in self.mapping:\n",
    "                    # Get relevant texts for this label\n",
    "                    label_texts = [text for l, text in zip(labels, texts) if l == label]\n",
    "                    if not label_texts:  # If no texts found, use empty list\n",
    "                        label_texts = ['']\n",
    "                    self.mapping[label] = self._find_most_similar_label(label, label_texts)\n",
    "                processed_labels.append(self.mapping[label])\n",
    "            else:\n",
    "                processed_labels.append(label)\n",
    "        \n",
    "        return self.encoder.transform(processed_labels)\n",
    "    \n",
    "    def inverse_transform(self, indices: np.ndarray) -> np.ndarray:\n",
    "        return self.encoder.inverse_transform(indices)\n",
    "    \n",
    "    @property\n",
    "    def classes_(self) -> np.ndarray:\n",
    "        return self.encoder.classes_\n",
    "\n",
    "class CybercrimeDataset(Dataset):\n",
    "    \"\"\"Custom dataset for cybercrime text classification\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        texts: List[str],\n",
    "        labels: List[int] = None,\n",
    "        tokenizer = None,\n",
    "        max_length: int = 128\n",
    "    ):\n",
    "        # Convert texts to list and handle NaN values\n",
    "        self.texts = [str(text) for text in texts]\n",
    "        \n",
    "        # Pre-tokenize all texts at once\n",
    "        self.encodings = tokenizer(\n",
    "            self.texts,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Convert labels to Long tensor if provided\n",
    "        if labels is not None:\n",
    "            self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        else:\n",
    "            self.labels = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "        }\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            item['labels'] = self.labels[idx]\n",
    "            \n",
    "        return item\n",
    "\n",
    "class CybercrimeClassifier:\n",
    "    \"\"\"Main classifier class with enhanced training and evaluation\"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.device = config.device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        self.model = None\n",
    "        self.label_encoder = SmartLabelEncoder()\n",
    "        \n",
    "    def prepare_data(self) -> Tuple[DataLoader, DataLoader]:\n",
    "        \"\"\"Load and prepare train and test data\"\"\"\n",
    "        logger.info(\"Loading datasets...\")\n",
    "        \n",
    "        # Load data\n",
    "        train_df = pd.read_csv(self.config.train_path)\n",
    "        test_df = pd.read_csv(self.config.test_path)\n",
    "        \n",
    "        # Convert text columns to string and fill NaN values\n",
    "        train_df[self.config.text_column] = train_df[self.config.text_column].fillna('').astype(str)\n",
    "        test_df[self.config.text_column] = test_df[self.config.text_column].fillna('').astype(str)\n",
    "        \n",
    "        # Log dataset statistics\n",
    "        logger.info(f\"Training set size: {len(train_df)}\")\n",
    "        logger.info(f\"Test set size: {len(test_df)}\")\n",
    "        logger.info(\"\\nTraining set label distribution:\")\n",
    "        for label, count in train_df[self.config.label_column].value_counts().items():\n",
    "            logger.info(f\"{label}: {count}\")\n",
    "        \n",
    "        # Check for unseen labels\n",
    "        train_labels = set(train_df[self.config.label_column].unique())\n",
    "        test_labels = set(test_df[self.config.label_column].unique())\n",
    "        unseen_labels = test_labels - train_labels\n",
    "        if unseen_labels:\n",
    "            logger.warning(f\"Found {len(unseen_labels)} unseen labels in test set: {unseen_labels}\")\n",
    "        \n",
    "        # Encode labels using smart encoder\n",
    "        self.label_encoder.fit(\n",
    "            train_df[self.config.label_column].tolist(),\n",
    "            train_df[self.config.text_column].tolist()\n",
    "        )\n",
    "        \n",
    "        train_labels = self.label_encoder.transform(\n",
    "            train_df[self.config.label_column].tolist(),\n",
    "            train_df[self.config.text_column].tolist()\n",
    "        ).astype(np.int64)\n",
    "        \n",
    "        test_labels = self.label_encoder.transform(\n",
    "            test_df[self.config.label_column].tolist(),\n",
    "            test_df[self.config.text_column].tolist()\n",
    "        ).astype(np.int64)\n",
    "        \n",
    "        num_labels = len(self.label_encoder.classes_)\n",
    "        logger.info(f\"Number of unique labels: {num_labels}\")\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.config.model_name,\n",
    "            num_labels=num_labels,\n",
    "            problem_type=\"single_label_classification\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = CybercrimeDataset(\n",
    "            texts=train_df[self.config.text_column].tolist(),\n",
    "            labels=train_labels,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        test_dataset = CybercrimeDataset(\n",
    "            texts=test_df[self.config.text_column].tolist(),\n",
    "            labels=test_labels,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_length=self.config.max_length\n",
    "        )\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=False\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=False\n",
    "        )\n",
    "        \n",
    "        return train_loader, test_loader\n",
    "        \n",
    "    def train(self, train_loader: DataLoader) -> None:\n",
    "        \"\"\"Balanced training loop for speed and accuracy\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Starting training...\")\n",
    "\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                self.model.parameters(),\n",
    "                lr=self.config.learning_rate,\n",
    "                weight_decay=self.config.weight_decay,\n",
    "                betas=(0.9, 0.999)\n",
    "            )\n",
    "\n",
    "            total_steps = len(train_loader) * self.config.num_epochs\n",
    "\n",
    "            scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "                optimizer,\n",
    "                max_lr=self.config.learning_rate,\n",
    "                total_steps=total_steps,\n",
    "                pct_start=0.1\n",
    "            )\n",
    "\n",
    "            best_loss = float('inf')\n",
    "            best_accuracy = 0.0\n",
    "\n",
    "            # Track overall progress\n",
    "            overall_progress = tqdm(total=self.config.num_epochs, desc=\"Overall Progress\")\n",
    "\n",
    "            for epoch in range(self.config.num_epochs):\n",
    "                logger.info(f\"\\nStarting Epoch {epoch + 1}/{self.config.num_epochs}\")\n",
    "                self.model.train()\n",
    "                running_loss = 0\n",
    "                running_correct = 0\n",
    "                running_total = 0\n",
    "\n",
    "                # Create progress bar for this epoch\n",
    "                progress_bar = tqdm(\n",
    "                    enumerate(train_loader),\n",
    "                    total=len(train_loader),\n",
    "                    desc=f'Epoch {epoch + 1}/{self.config.num_epochs}',\n",
    "                    ncols=100,\n",
    "                    leave=False  # Don't leave the progress bar\n",
    "                )\n",
    "\n",
    "                try:\n",
    "                    for batch_idx, batch in progress_bar:\n",
    "                        # Zero gradients\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        # Move batch to device\n",
    "                        batch = {k: v.to(self.device, non_blocking=True) for k, v in batch.items()}\n",
    "\n",
    "                        # Forward pass\n",
    "                        outputs = self.model(**batch)\n",
    "                        loss = outputs.loss / self.config.gradient_accumulation_steps\n",
    "\n",
    "                        # Backward pass\n",
    "                        loss.backward()\n",
    "\n",
    "                        # Gradient accumulation\n",
    "                        if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:\n",
    "                            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                            optimizer.step()\n",
    "                            scheduler.step()\n",
    "                            optimizer.zero_grad()\n",
    "\n",
    "                        # Calculate accuracy\n",
    "                        with torch.no_grad():\n",
    "                            predictions = outputs.logits.argmax(-1)\n",
    "                            correct = (predictions == batch['labels']).sum().item()\n",
    "                            total = batch['labels'].size(0)\n",
    "                            running_correct += correct\n",
    "                            running_total += total\n",
    "\n",
    "                        running_loss += loss.item() * self.config.gradient_accumulation_steps\n",
    "\n",
    "                        # Update progress\n",
    "                        if (batch_idx + 1) % 10 == 0:\n",
    "                            avg_loss = running_loss / (batch_idx + 1)\n",
    "                            accuracy = (running_correct / running_total) * 100\n",
    "\n",
    "                            progress_bar.set_postfix({\n",
    "                                'loss': f'{avg_loss:.4f}',\n",
    "                                'acc': f'{accuracy:.2f}%'\n",
    "                            }, refresh=True)\n",
    "\n",
    "                        # Save best model\n",
    "                        if batch_idx > 0 and batch_idx % self.config.eval_steps == 0:\n",
    "                            current_accuracy = (running_correct / running_total) * 100\n",
    "                            if current_accuracy > best_accuracy:\n",
    "                                best_accuracy = current_accuracy\n",
    "                                self.save_model('best_model.pt')\n",
    "                                logger.info(f\"New best model saved with accuracy: {best_accuracy:.2f}%\")\n",
    "\n",
    "                    # Epoch end validation\n",
    "                    val_accuracy = self.quick_evaluate(train_loader)\n",
    "                    logger.info(f\"Epoch {epoch + 1} completed. Validation accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "                    # Save checkpoint after each epoch\n",
    "                    checkpoint_path = f'checkpoint_epoch_{epoch+1}.pt'\n",
    "                    self.save_model(checkpoint_path)\n",
    "                    logger.info(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "                    # Reset metrics\n",
    "                    running_loss = 0\n",
    "                    running_correct = 0\n",
    "                    running_total = 0\n",
    "\n",
    "                    # Update overall progress\n",
    "                    overall_progress.update(1)\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error during epoch {epoch + 1}: {str(e)}\")\n",
    "                    logger.error(traceback.format_exc())\n",
    "                    # Save emergency checkpoint\n",
    "                    self.save_model(f'emergency_checkpoint_epoch_{epoch+1}.pt')\n",
    "                    raise\n",
    "\n",
    "            overall_progress.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Training error: {str(e)}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            raise\n",
    "        finally:\n",
    "            # Save final model even if there was an error\n",
    "            try:\n",
    "                self.save_model('final_model.pt')\n",
    "                logger.info(\"Final model saved\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error saving final model: {str(e)}\")\n",
    "\n",
    "    def quick_evaluate(self, loader: DataLoader) -> float:\n",
    "        \"\"\"Quick evaluation during training\"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                batch = {k: v.to(self.device, non_blocking=True) for k, v in batch.items()}\n",
    "                outputs = self.model(**batch)\n",
    "                predictions = outputs.logits.argmax(-1)\n",
    "                correct += (predictions == batch['labels']).sum().item()\n",
    "                total += batch['labels'].size(0)\n",
    "\n",
    "        return (correct / total) * 100\n",
    "\n",
    "\n",
    "                \n",
    "    def evaluate(self, test_loader: DataLoader) -> None:\n",
    "        \"\"\"Evaluate the model on test data with improved error handling\"\"\"\n",
    "        logger.info(\"Evaluating model...\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        # Collect predictions\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                outputs = self.model(**batch)\n",
    "                preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(batch['labels'].cpu().numpy())\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        all_preds = np.array(all_preds)\n",
    "        all_labels = np.array(all_labels)\n",
    "        \n",
    "        # Get unique classes in predictions and true labels\n",
    "        unique_preds = np.unique(all_preds)\n",
    "        unique_labels = np.unique(all_labels)\n",
    "        \n",
    "        # Print diagnostics\n",
    "        print(\"\\nDiagnostics:\")\n",
    "        print(f\"Number of unique classes in predictions: {len(unique_preds)}\")\n",
    "        print(f\"Number of unique classes in true labels: {len(unique_labels)}\")\n",
    "        print(f\"Number of classes in label encoder: {len(self.label_encoder.classes_)}\")\n",
    "        \n",
    "        print(\"\\nClass distribution in predictions:\")\n",
    "        pred_counts = Counter(all_preds)\n",
    "        for class_idx, count in sorted(pred_counts.items()):\n",
    "            class_name = self.label_encoder.inverse_transform([class_idx])[0]\n",
    "            print(f\"Class {class_idx} ({class_name}): {count} samples\")\n",
    "        \n",
    "        print(\"\\nClass distribution in true labels:\")\n",
    "        label_counts = Counter(all_labels)\n",
    "        for class_idx, count in sorted(label_counts.items()):\n",
    "            class_name = self.label_encoder.inverse_transform([class_idx])[0]\n",
    "            print(f\"Class {class_idx} ({class_name}): {count} samples\")\n",
    "        \n",
    "        # Get the intersection of classes present in both predictions and true labels\n",
    "        present_classes = sorted(set(unique_preds) & set(unique_labels))\n",
    "        class_names = self.label_encoder.inverse_transform(present_classes)\n",
    "        \n",
    "        # Print classification report with only present classes\n",
    "        print(\"\\nClassification Report:\")\n",
    "        try:\n",
    "            report = classification_report(\n",
    "                all_labels,\n",
    "                all_preds,\n",
    "                labels=present_classes,\n",
    "                target_names=class_names,\n",
    "                digits=4,\n",
    "                zero_division=0\n",
    "            )\n",
    "            print(report)\n",
    "            \n",
    "            # Print confusion matrix\n",
    "            cm = confusion_matrix(all_labels, all_preds, labels=present_classes)\n",
    "            print(\"\\nConfusion Matrix:\")\n",
    "            print(\"Labels:\", class_names.tolist())\n",
    "            print(cm)\n",
    "            \n",
    "            # Calculate and print accuracy\n",
    "            accuracy = (all_preds == all_labels).mean() * 100\n",
    "            print(f\"\\nOverall Accuracy: {accuracy:.2f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in generating classification report: {str(e)}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "    \n",
    "    # Update the CybercrimeClassifier class with the new evaluate method\n",
    "    \n",
    "        \n",
    "    def predict(self, text: str) -> Tuple[str, float]:\n",
    "        \"\"\"Predict category for a single text\"\"\"\n",
    "        self.model.eval()\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.config.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        encoding = {k: v.to(self.device) for k, v in encoding.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**encoding)\n",
    "            probs = F.softmax(outputs.logits, dim=1)\n",
    "            predicted_class = torch.argmax(probs, dim=1).cpu().numpy()[0]\n",
    "            confidence = probs[0][predicted_class].cpu().numpy()\n",
    "        \n",
    "        predicted_category = self.label_encoder.inverse_transform([predicted_class])[0]\n",
    "        return predicted_category, confidence\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def save_model(self, path: str) -> None:\n",
    "        \"\"\"Save model and label encoder\"\"\"\n",
    "        save_dict = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'label_encoder': self.label_encoder,\n",
    "            'config': self.config,\n",
    "        }\n",
    "        torch.save(save_dict, path)\n",
    "        logger.info(f\"Model saved to {path}\")\n",
    "\n",
    "    def load_model(self, path: str) -> None:\n",
    "        \"\"\"Load model and label encoder\"\"\"\n",
    "        save_dict = torch.load(path, map_location=self.device)\n",
    "        self.model.load_state_dict(save_dict['model_state_dict'])\n",
    "        self.label_encoder = save_dict['label_encoder']\n",
    "        logger.info(f\"Model loaded from {path}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    try:\n",
    "        # Initialize configuration\n",
    "        config = Config()\n",
    "        logger.info(f\"Using device: {config.device}\")\n",
    "        \n",
    "        # Initialize classifier\n",
    "        classifier = CybercrimeClassifier(config)\n",
    "        \n",
    "        # Prepare data\n",
    "        logger.info(\"Preparing data...\")\n",
    "        train_loader, test_loader = classifier.prepare_data()\n",
    "        \n",
    "        # Train model\n",
    "        logger.info(\"Starting training process...\")\n",
    "        classifier.train(train_loader)\n",
    "        \n",
    "        # Save final model\n",
    "        classifier.save_model(config.output_dir / 'final_model.pt')\n",
    "        \n",
    "        # Evaluate model\n",
    "        logger.info(\"Evaluating model...\")\n",
    "        classifier.evaluate(test_loader)\n",
    "        \n",
    "        # Example prediction\n",
    "        sample_text = \"\"\"I received a call from someone claiming to be from SBI bank asking for my card details. \n",
    "        They said my card would be blocked. I gave them my details and lost Rs. 50,000.\"\"\"\n",
    "        \n",
    "        logger.info(\"\\nTesting model with sample prediction...\")\n",
    "        predicted_category, confidence = classifier.predict(sample_text)\n",
    "        print(f\"\\nSample Prediction:\")\n",
    "        print(f\"Text: {sample_text[:100]}...\")\n",
    "        print(f\"Predicted Category: {predicted_category}\")\n",
    "        print(f\"Confidence: {confidence:.2%}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {str(e)}\")\n",
    "        logger.error(f\"Stack trace: {traceback.format_exc()}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "    \n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8bbb63-f94f-4f4c-8b5f-2b980ff4a647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe30289-e2d7-4f6d-b7fc-1eab037f4ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d291ef7f-d921-44bd-9fd7-2339ff8e2bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a99a1b3-7279-4fa1-9a46-f19d94a3fc0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b003b807-c6b4-4c3c-bb0b-20d3200c5ffb",
   "metadata": {},
   "source": [
    "# Running the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81edfefd-9a8b-4f5d-853b-1a616531116e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e197e5e0-c479-45ad-ba02-e63faf1ecaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 17:17:20,335 - INFO - Using device: cuda\n",
      "2024-11-07 17:17:20,336 - INFO - Loading model from final_model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Cybercrime Classification System...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kollm\\AppData\\Local\\Temp\\ipykernel_26748\\3802240344.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  saved_data = torch.load(model_path, map_location=self.device)\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-11-07 17:17:22,325 - INFO - Model loaded successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cybercrime Classification Menu:\n",
      "1. Test with custom input\n",
      "2. Test with examples from test dataset\n",
      "3. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your choice (1-3):  1\n",
      "\n",
      "Enter the text to classify (or 'q' to return to menu):  q\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cybercrime Classification Menu:\n",
      "1. Test with custom input\n",
      "2. Test with examples from test dataset\n",
      "3. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your choice (1-3):  1\n",
      "\n",
      "Enter the text to classify (or 'q' to return to menu):  I received a call from someone claiming to be from SBI bank asking for my card details.          They said my card would be blocked. I gave them my details and lost Rs. 50,000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction Results:\n",
      "==================================================\n",
      "Input Text: I received a call from someone claiming to be from SBI bank asking for my card details.          They said my card would be blocked. I gave them my details and lost Rs. 50,000.\n",
      "Predicted Category: Online Financial Fraud\n",
      "Confidence: 98.91%\n",
      "==================================================\n",
      "\n",
      "Cybercrime Classification Menu:\n",
      "1. Test with custom input\n",
      "2. Test with examples from test dataset\n",
      "3. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your choice (1-3):  2\n",
      "\n",
      "How many random examples to test? (press Enter for all):  10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing 10 examples from cleaned_cybercrime_test_data.csv\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "303b96f9b6e04d988101870f8f23014d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing examples:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Overall Statistics ===\n",
      "Total Examples: 10\n",
      "Correct Predictions: 7\n",
      "Wrong Predictions: 3\n",
      "Overall Accuracy: 70.00%\n",
      "\n",
      "=== Category-wise Statistics ===\n",
      "\n",
      "Online Financial Fraud (7 cases):\n",
      "├── Correct: 6\n",
      "├── Wrong: 1\n",
      "├── Accuracy: 85.71%\n",
      "└── Avg Confidence: 0.88%\n",
      "\n",
      "Online and Social Media Related Crime (1 cases):\n",
      "├── Correct: 0\n",
      "├── Wrong: 1\n",
      "├── Accuracy: 0.00%\n",
      "└── Avg Confidence: 0.75%\n",
      "\n",
      "Cyber Attack/ Dependent Crimes (1 cases):\n",
      "├── Correct: 1\n",
      "├── Wrong: 0\n",
      "├── Accuracy: 100.00%\n",
      "└── Avg Confidence: 1.00%\n",
      "\n",
      "Sexually Explicit Act (1 cases):\n",
      "├── Correct: 0\n",
      "├── Wrong: 1\n",
      "├── Accuracy: 0.00%\n",
      "└── Avg Confidence: 0.81%\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "\n",
      "True Category: Online Financial Fraud\n",
      "├── Predicted as Online Financial Fraud: 6 (85.7%)\n",
      "├── Predicted as Hacking  Damage to computercomputer system etc: 1 (14.3%)\n",
      "\n",
      "True Category: Online and Social Media Related Crime\n",
      "├── Predicted as Online Financial Fraud: 1 (100.0%)\n",
      "\n",
      "True Category: Cyber Attack/ Dependent Crimes\n",
      "├── Predicted as Cyber Attack/ Dependent Crimes: 1 (100.0%)\n",
      "\n",
      "True Category: Sexually Explicit Act\n",
      "├── Predicted as Online and Social Media Related Crime: 1 (100.0%)\n",
      "\n",
      "Cybercrime Classification Menu:\n",
      "1. Test with custom input\n",
      "2. Test with examples from test dataset\n",
      "3. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your choice (1-3):  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thank you for using the Cybercrime Classification System! By Muhammad Mamoon jan\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, List, Union\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.model_name = 'distilbert-base-uncased'\n",
    "        self.max_length = 256\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.test_path = 'cleaned_cybercrime_test_data.csv'  # Path to test data if available and change to differnt test\n",
    "\n",
    "class CybercrimePredictor:\n",
    "    def __init__(self, model_path: str):\n",
    "        \"\"\"Initialize the predictor with a saved model\"\"\"\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Load saved model data\n",
    "        logger.info(f\"Loading model from {model_path}\")\n",
    "        saved_data = torch.load(model_path, map_location=self.device)\n",
    "        \n",
    "        # Get configuration\n",
    "        self.config = saved_data['config']\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.config.model_name,\n",
    "            num_labels=len(saved_data['label_encoder'].classes_)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Load model weights and label encoder\n",
    "        self.model.load_state_dict(saved_data['model_state_dict'])\n",
    "        self.label_encoder = saved_data['label_encoder']\n",
    "        self.model.eval()\n",
    "        \n",
    "        logger.info(\"Model loaded successfully!\")\n",
    "\n",
    "    def predict(self, text: str) -> Tuple[str, float]:\n",
    "        \"\"\"Predict the category of a given text\"\"\"\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.config.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        encoding = {k: v.to(self.device) for k, v in encoding.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**encoding)\n",
    "            probs = F.softmax(outputs.logits, dim=1)\n",
    "            predicted_class = torch.argmax(probs, dim=1).cpu().numpy()[0]\n",
    "            confidence = probs[0][predicted_class].cpu().numpy()\n",
    "        \n",
    "        predicted_category = self.label_encoder.inverse_transform([predicted_class])[0]\n",
    "        return predicted_category, confidence\n",
    "\n",
    "def test_custom_input(predictor: CybercrimePredictor, text: str) -> None:\n",
    "    \"\"\"Test the model with custom user input\"\"\"\n",
    "    predicted_category, confidence = predictor.predict(text)\n",
    "    print(\"\\nPrediction Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Input Text: {text}\")\n",
    "    print(f\"Predicted Category: {predicted_category}\")\n",
    "    print(f\"Confidence: {confidence:.2%}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "def test_from_csv(predictor: CybercrimePredictor, csv_path: str, num_samples: Union[int, None] = None) -> None:\n",
    "    \"\"\"Test the model with examples from a CSV file with detailed statistics\"\"\"\n",
    "    test_df = pd.read_csv(csv_path)\n",
    "    \n",
    "    if num_samples:\n",
    "        test_df = test_df.sample(n=min(num_samples, len(test_df)), random_state=42)\n",
    "    \n",
    "    print(f\"\\nTesting {len(test_df)} examples from {csv_path}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    correct = 0\n",
    "    results = []\n",
    "    category_stats = {}\n",
    "    confusion_matrix = {}\n",
    "    \n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing examples\"):\n",
    "        text = str(row['crimeaditionalinfo'])\n",
    "        true_category = row['category']\n",
    "        predicted_category, confidence = predictor.predict(text)\n",
    "        \n",
    "        # Initialize statistics\n",
    "        if true_category not in category_stats:\n",
    "            category_stats[true_category] = {'total': 0, 'correct': 0, 'wrong': 0, 'confidences': []}\n",
    "        if true_category not in confusion_matrix:\n",
    "            confusion_matrix[true_category] = {}\n",
    "        if predicted_category not in confusion_matrix[true_category]:\n",
    "            confusion_matrix[true_category][predicted_category] = 0\n",
    "            \n",
    "        # Update statistics\n",
    "        confusion_matrix[true_category][predicted_category] += 1\n",
    "        category_stats[true_category]['total'] += 1\n",
    "        \n",
    "        is_correct = predicted_category == true_category\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "            category_stats[true_category]['correct'] += 1\n",
    "        else:\n",
    "            category_stats[true_category]['wrong'] += 1\n",
    "            \n",
    "        category_stats[true_category]['confidences'].append(float(confidence))\n",
    "        \n",
    "        results.append({\n",
    "            'text': text[:100] + \"...\" if len(text) > 100 else text,\n",
    "            'true_category': true_category,\n",
    "            'predicted_category': predicted_category,\n",
    "            'confidence': confidence,\n",
    "            'is_correct': is_correct\n",
    "        })\n",
    "    \n",
    "    # Print statistics\n",
    "    print_statistics(results, category_stats, confusion_matrix)\n",
    "\n",
    "def print_statistics(results: List[Dict], category_stats: Dict, confusion_matrix: Dict) -> None:\n",
    "    \"\"\"Print detailed statistics of the model's performance\"\"\"\n",
    "    total = len(results)\n",
    "    correct = sum(1 for r in results if r['is_correct'])\n",
    "    accuracy = (correct / total) * 100\n",
    "    \n",
    "    print(\"\\n=== Overall Statistics ===\")\n",
    "    print(f\"Total Examples: {total}\")\n",
    "    print(f\"Correct Predictions: {correct}\")\n",
    "    print(f\"Wrong Predictions: {total - correct}\")\n",
    "    print(f\"Overall Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    # Category-wise statistics\n",
    "    print(\"\\n=== Category-wise Statistics ===\")\n",
    "    for category, stats in sorted(category_stats.items(), key=lambda x: x[1]['total'], reverse=True):\n",
    "        cat_accuracy = (stats['correct'] / stats['total'] * 100) if stats['total'] > 0 else 0\n",
    "        avg_confidence = sum(stats['confidences']) / len(stats['confidences'])\n",
    "        \n",
    "        print(f\"\\n{category} ({stats['total']} cases):\")\n",
    "        print(f\"├── Correct: {stats['correct']}\")\n",
    "        print(f\"├── Wrong: {stats['wrong']}\")\n",
    "        print(f\"├── Accuracy: {cat_accuracy:.2f}%\")\n",
    "        print(f\"└── Avg Confidence: {avg_confidence:.2f}%\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    print(\"\\n=== Confusion Matrix ===\")\n",
    "    for true_cat in confusion_matrix:\n",
    "        print(f\"\\nTrue Category: {true_cat}\")\n",
    "        for pred_cat, count in sorted(confusion_matrix[true_cat].items(), key=lambda x: x[1], reverse=True):\n",
    "            if count > 0:\n",
    "                percentage = (count / category_stats[true_cat]['total']) * 100\n",
    "                print(f\"├── Predicted as {pred_cat}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "def interactive_testing():\n",
    "    \"\"\"Interactive interface for testing the model\"\"\"\n",
    "    print(\"Initializing Cybercrime Classification System...\")\n",
    "    \n",
    "    try:\n",
    "        model_path = 'final_model.pt'\n",
    "        predictor = CybercrimePredictor(model_path)\n",
    "        \n",
    "        while True:\n",
    "            print(\"\\nCybercrime Classification Menu:\")\n",
    "            print(\"1. Test with custom input\")\n",
    "            print(\"2. Test with examples from test dataset\")\n",
    "            print(\"3. Exit\")\n",
    "            \n",
    "            choice = input(\"\\nEnter your choice (1-3): \")\n",
    "            \n",
    "            if choice == '1':\n",
    "                text = input(\"\\nEnter the text to classify (or 'q' to return to menu): \")\n",
    "                if text.lower() == 'q':\n",
    "                    continue\n",
    "                test_custom_input(predictor, text)\n",
    "                \n",
    "            elif choice == '2':\n",
    "                if not os.path.exists('cleaned_cybercrime_test_data.csv'):\n",
    "                    print(\"\\nTest dataset not found! Please place 'cleaned_cybercrime_test_data.csv' in the current directory.\")\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    num_samples = input(\"\\nHow many random examples to test? (press Enter for all): \")\n",
    "                    num_samples = int(num_samples) if num_samples.strip() else None\n",
    "                    test_from_csv(predictor, 'cleaned_cybercrime_test_data.csv', num_samples)\n",
    "                except ValueError:\n",
    "                    print(\"Please enter a valid number\")\n",
    "                \n",
    "            elif choice == '3':\n",
    "                print(\"\\nThank you for using the Cybercrime Classification System! By Muhammad Mamoon jan\")\n",
    "                break\n",
    "                \n",
    "            else:\n",
    "                print(\"Invalid choice! Please enter 1, 2, or 3.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {str(e)}\")\n",
    "        logger.error(\"Please make sure all required files are present and valid.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    interactive_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53cac18-56c4-4af5-8b53-487cc4275d20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71e0f3d-f05f-477c-ac5f-7e6e8841523f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0ebd95-181d-4ad9-8510-b231a7092f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1132d02-88cc-42de-a47e-0e47bc93760b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (C_gpu2)",
   "language": "python",
   "name": "c_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
